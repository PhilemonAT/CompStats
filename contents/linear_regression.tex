\section{Linear Regression}
Linear Model: $Y = X\beta + \epsilon$, $Y \in \mathbb{R}^n, X \in \mathbb{R}^{n\times p}, \beta \in \mathbb{R}^p$
\subsection*{Definitions and important results}
$RSS(\beta) = ||Y-X\beta||_2^2$, $\beta \in \mathbb{R}^p$; Residuals: $r \coloneqq Y - \hat{Y}$ \\
\textbf{Estimated params.}: $\hat{\beta} = \mathcolor{gray}{(X^\intercal X)^{-1}X^\intercal Y}$; \\
$\hat{Y} = \mathcolor{gray}{X\hat{\beta} = X(X^\intercal X)^{-1}X^\intercal  Y}$; $\hat{Y}_{new} = \mathcolor{gray}{X_{new}^\intercal \hat{\beta}}$\\
\textbf{Projection} $P \coloneqq X(X^\intercal  X)^{-1}X^\intercal $ is projection onto column space of $X$ $\implies \forall j \in {1,...,p}: r^\intercal x^j = 0$ \\
\textbf{Inference} Consider $Y = X\beta + \epsilon$, $\mathbb{E}[\epsilon]=0, Cov(\epsilon)=\mathbb{E}[\epsilon\epsilon^\intercal ]=\sigma^2\mathds{1}_{n\times n}, rank(X^\intercal X)=p$. Then, \\
\vspace{-0.8\baselineskip}
\begin{itemize}
    \item $\mathbb{E}[\hat\beta] = \mathbb{E}[(X^\intercal X)^{-1}X^\intercal (X\beta + \epsilon)]=\beta$ \textrightarrow $\hat{\beta}$ unbiased
    \item $\mathbb{E}[\hat{Y}] = \mathbb{E}[Y] = X\beta$ and $\mathbb{E}[r] = 0$ 
    \item $Cov(\hat{\beta}) = \sigma^2(X^\intercal X)^{-1}$; $Cov(\hat{Y}) = \sigma^2P$
    \item $Cov(r) = \sigma^2(\mathds{1}-P)$ \textrightarrow In general non-diagonal!
    \item $\mathbb{E}[\sum_{i=1}^n r_i^2] = \sum_{i=1}^n \mathbb{E}[r_i^2] = \sum_{i=1}^n \sigma^2(\mathds{1}-P_{ii}) = \sigma^2(n-tr(P)) = \\ \sigma^2(n-p)$ \textrightarrow $\hat{\sigma}^2 = (n-p)^{-1}\sum_{i=1}^n r_i^2$ unbiased for $\sigma ^2$
\end{itemize}
\textbf{Normality}: Assume now $\epsilon_1,...,\epsilon_n \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$. Then,
\begin{itemize}
    \item $\hat{\beta} \sim \mathcal{N}(\beta, \sigma^2(X^\intercal X)^{-1})$; $\hat{Y} \sim \mathcal{N}(X\beta, \sigma^2P)$ and $r \sim \mathcal{N}(0,\sigma^2(\mathds{1}-P))$
    \item $\hat{\sigma}^2 \sim \frac{\sigma^2}{n-p}\mathcal{X}_{n-p}^2$
\end{itemize}
\subsection*{Tests and confidence intervals}
Assume again \textbf{normality}. Null hyp.: $H_{0, j}: \beta_j = 0$ \\
Under the null: $T_j = \frac{\hat{\beta}_j}{\sqrt{\hat{\sigma}^2[(X^\intercal X)^{-1}]_{jj}}} \sim t_{n-p}$\\
\textbf{Note}: An \emph{individual} t-test for $H_{0,j}$ quantifies the effect of j-th predictor var. after having subtracted the linear effect of all other predictors on Y\\
\textbf{Confidence Interval} $\hat{\beta}_j \pm \sqrt{\hat{\sigma}^2[(X^\intercal X)^{-1}]_{jj}}\cdot t_{n-p;1-\alpha/2}$\\
\textbf{CI and PI for $x_0^\intercal \beta$}: CI $= \frac{x_0^\intercal \hat{\beta}-x_0^\intercal \beta}{\hat{\sigma}\sqrt{x_0^\intercal (X^\intercal X)^{-1}x_0}} \sim t_{n-p}$ \\ and PI = $\frac{y_0 - x_0^\intercal \beta}{\hat{\sigma}\sqrt{1+x_0^\intercal (X^\intercal X)^{-1}x_0}} \sim t_{n-p}$

\textbf{Global}: $H_0:\beta_2,...,\beta_p=0$ vs. $H_A:\exists j \in {2,...,p}:\beta_j \neq 0$\\
Anova Decomp.: $||Y-\Bar{Y}||^2=||Y-\hat{Y}||^2+ \mathcolor{purple}{||\hat{Y} - \Bar{Y}||^2}$, where under $H_0$, \textcolor{purple}{this} is $\approx 0$. Also, we have $R^2 \coloneqq \frac{||\hat{Y} - \Bar{Y}||^2}{||Y-\Bar{Y}||^2}$. In SLR, this is : $R^2 = (\text{cor}(Y, \hat{Y}))^2$\\
\textbf{Anova table}:
\vspace{-\baselineskip}
{\tiny
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Source & df & Sum of Squares & Mean Square \\
\hline
Regression & \(p-1\) & \(||\hat{Y}-\bar{Y}||^2\) & \(\frac{||\hat{Y}-\bar{Y}||^2}{p-1}\) \\
Error & \(n-p\) & \(||Y-\hat{Y}||^2\) & \(\frac{||Y-\hat{Y}||^2}{n-p}\) \\
\hline
Total & \(n-1\) & \(||Y-\bar{Y}||^2\) & \\
\hline
\end{tabular}
\end{center}
}
\vspace{-\baselineskip}
\textbf{F-test}: $\frac{||\hat{Y}-\Bar{Y}||^2/(p-1)}{||Y-\hat{Y}||^2/(n-p)} \sim F_{p-1, n-p}$. Comp. alternative: Permutation test\\
\textbf{Def. level} Let $\alpha \in (0,1)$ and $\forall P \in H_0$ let $P^n \coloneqq P \otimes ... \otimes P$. A test $T_n:\mathcal{X}^n \to \{0,1\}$ (where $T_n=1$ means that we reject $H_0$) is a test with...
\begin{enumerate}
    \item \textcolor{red}{level} $\mathcolor{red}{\alpha}$ \textcolor{red}{if} $\sup_{P\in H_0} \mathbb{P}_{P^n}(T_n=1) \leq \alpha$
    \item \textcolor{blue}{pointwise asymptotic level} $\mathcolor{blue}{\alpha}$ \textcolor{blue}{if}\\ $\sup_{P\in H_0}\lim_{n\to \infty}\mathbb{P}_{P^n}(T_n=1) \leq \alpha$
    \item \textcolor{orange}{uniform asymptotic level} $\mathcolor{orange}{\alpha}$ \textcolor{orange}{if}\\ $\lim_{n\to \infty}\sup_{P\in H_0}\mathbb{P}_{P^n}(T_n=1)\leq \alpha$
\end{enumerate}
\emph{Note}: 2 weaker than 3 weaker than 1

\textbf{p-value}: Given a test, the p-value is the infimum over all significance levels s.t. the test rejects
\subsection*{Evaluating model assumptions}
\begin{itemize}
    \item TA-plot: $r_i$ vs. $\hat{Y}_i$ (sample corr. btw. $\hat{Y}$ \& $r_i$ is = 0) 
    \item QQ-plot: emp. quantile of (standardized) residuals $(Y_i -\hat{Y}_i)/\hat{\sigma}$ vs. th. quantile of  $\mathcal{N}(0,1)$ \textrightarrow Too many (few) large values: right-(left-)skewed
    \item Cook's distance: $D_i \coloneqq \frac{\sum_{j+1}(\hat{Y}_j-\widehat{Y_j^{-i}})^2}{p||Y-\hat{Y}||^2/(n-p)}$ ($D_i>1$ infl.)
\end{itemize}
\subsection*{Model selection}
If $p<n$ but $p\approx n$, we may be able to \textbf{decrease aMSE} (not RSS!): \\ 
Let $J_q \coloneqq \{j_1,...,j_q\}, \#J_q=q\leq p$. With $\hat{m}_{J_q}(\cdot)$ the smaller model fitted with predictors $J_q$, we have $aMSE = \frac{1}{n}\sum_{i=1}^n\mathbb{E}[(\hat{m}_{J_q}(x_i)-m(x_i))^2] \\ 
=\frac{1}{n}\sum_{i=1}^n(\mathbb{E}[\hat{m}_{J_q}(x_i)]-m(x_i))^2 + \frac{1}{n}\sum_{i=1}^n\text{Var}(\hat{m}_{J_q}(x_i)) \\ 
=\frac{1}{n}\sum_{i=1}^n\text{Bias}(\hat{m}_{J_q}(x_i))^2 + \frac{q}{n}\sigma^2$ \textrightarrow var increases with q \\
Instead of incl. all variables, define $\forall \lambda \geq 0$: \\
$\hat{\beta}(\lambda) \coloneqq argmin_{\beta}||Y-X\beta||^2+\lambda||\beta||_0$, $||\beta||_0 \coloneqq \{j:\beta_j\neq0\}$
Choose $\lambda$ with AIC ($\lambda=2\hat{\sigma}^2$; equiv. to Mallow's CP with linear Gaussian models), BIC ($\lambda=log(n)\hat{\sigma}^2$), or CV. 
This problem is non-convex: can use forward \& backward selection or combinations. 